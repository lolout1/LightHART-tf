# config/smartfallmm/distill.yaml
# Knowledge Distillation configuration for LightHART-TF

# Models
teacher_model: models.mm_transformer.MMTransformer
student_model: models.transformer_optimized.TransModel
model: models.transformer_optimized.TransModel  # Student model for API compatibility
dataset: smartfallmm

# Cross-validation subjects (same as in other configs)
subjects: [32, 39, 30, 31, 33, 34, 35, 37, 43, 44, 45, 36, 29]

# Teacher model configuration
teacher_args:
  mocap_frames: 128
  acc_frames: 128
  num_joints: 32
  in_chans: 3
  num_patch: 4
  acc_coords: 3
  spatial_embed: 16
  sdepth: 4
  adepth: 4
  tdepth: 2
  num_heads: 2
  mlp_ratio: 2
  qkv_bias: true
  op_type: 'pool'
  embed_type: 'lin'
  drop_rate: 0.2
  attn_drop_rate: 0.2
  drop_path_rate: 0.2
  num_classes: 1

# Student model configuration (accelerometer-only)
model_args:
  acc_frames: 128
  num_classes: 1
  num_heads: 4
  acc_coords: 3
  embed_dim: 32
  num_layers: 2
  dropout: 0.5
  activation: 'relu'

# Distillation parameters
distiller_args:
  temperature: 4.5  # Match PyTorch implementation
  alpha: 0.6  # Controls balance between feature distillation and classification loss

# Dataset configuration
dataset_args: 
  mode: 'sliding_window'
  max_length: 128
  task: 'fd'
  modalities: ['accelerometer', 'skeleton']  # Need both for teacher
  age_group: ['young']
  sensors: ['watch']
  use_dtw: true

# Training configuration
feeder: feeder.make_dataset_tf.UTD_MM_TF
batch_size: 16
test_batch_size: 16
val_batch_size: 16
num_epoch: 80
use_smv: false
optimizer: adamw
base_lr: 0.001
weight_decay: 0.0004

# Path to teacher weights
teacher_weight: ../experiments/mm_transformer/models/mm_transformer

# Work directory and logs
work_dir: ../experiments/distillation
phase: 'distill'
seed: 2
print_log: true
